{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project -1 <br>\n",
    "## Handwriting Digit Recognition with a Multi Layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by **Achyut Ganti** (G01676643) and \n",
    "             **Sizen Nuepane** (G01674480) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to code and train a Multilayered Percpetron to classify the digits from the MNIST dataset. The language of choice was Python and the code has been written using the Numpy and Pandas framework available in Python. The training dataset used to train the model has 40000 observations in it and the test data has 2000 observations in it.  \n",
    "\n",
    "Let us answer the following questions first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1Q. Describe the code. What approaches or other techniques did you use to improve the efficieny of the code.** <br>\n",
    "\n",
    "For both and the Sequential technique, \n",
    "The network consists of 784 input neurons, 500 hidden neurons and 10 output neurons.The code consists of several functions that works overall to build the multi-layer back propagation neural network. The functions used are:\n",
    "    \n",
    "    a) Initialize_network(args):\n",
    "This function assigns the random initial weights to the hidden layer and output layer. In both cases, the weight is also assigned to the bias.\n",
    "\n",
    "    b) Forward_propagate(args):\n",
    "This function calculates the output from the hidden layer and output layer. The matrix multiplication is used for fast computation. The output from each layer is sent to the sigmoid function to find the sigmoid approximation. \n",
    "\n",
    "    c) Sigmoid(df):\n",
    "Each output from hidden neurons and output neurons are sent to the function to calculate the sigmoid approximation.\n",
    "\n",
    "    d) Backward_propagate_error(args):\n",
    "This function calculates the error associated with the outputs from forward propagation. The errors are calculated for both output from output neurons and hidden neurons.\n",
    "\n",
    "    e) Deri_output(args):\n",
    "This function calculates the difference in the expected output and predicated output in output unit. This function is called from backward_propagate_error function. \n",
    "\n",
    "    f) Update_weights(args):\n",
    "This function updates the weights of network proportionately according to the calculated error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the batch technique \n",
    "\n",
    "The code consists of several functions that collectively helps in building the neural network. Some of the functions are \n",
    "\n",
    "\t- scaling(dF)  \n",
    "    \n",
    "Takes in a dataframe and normalizes it. \n",
    "    \n",
    "\t- initialize_weights(args)  \n",
    "    \n",
    "Used to Initialize the initial weights that initialize the neural network.\n",
    "    \n",
    "\t- sigmoid(args)\n",
    "    \n",
    "A sigmoid function that takes in a ndarray and applies the sigmoid function and returns the sigmoid approximation of that ndarray.\n",
    "    \n",
    "\t- randomize(df)\n",
    "    \n",
    "This is used to shuffle the data after every iteration. It takes in a dataframe and returns the shuffed dataframe. \n",
    "    \n",
    "\t- forward_prop(args)\n",
    "    \n",
    "This is where the forward propogation for the neural network happens. It takes in the input data and the weights as inputs and gives us the final Value of the neural network as the output along with other values that are obtained as a part of the forward propagation. \n",
    "    \n",
    "\t- error_function(args)\n",
    "    \n",
    "Calculates the errors associated with the outputs that were obtained by the forward propagation function. The outputs obtained are total error of the neural network, the error of the output unit of the neural network, and the error of the hidden_layer of the neural network. \n",
    "    \n",
    "\t- update_weights(args)\n",
    "    \n",
    "This function is used to update the weights . It updates the input_weights, output_weights and as well as those associated with the bias units. \n",
    "    \n",
    "After training is done, it is finally time to test the model on the test data and predict the output labels. And then we calculate the accuracy associated with the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the techniques that were used to improve the efficiency were \n",
    "- **Scaling the data**  - We had to scale the input data before using it to train the model. We have tried training the model without scaling the data and encountered an overflow warning which eventually converted all the output values that are closer to one to one. Further operations are impossible when tha happens. Could take care of that problem by scaling the data. \n",
    "- **Numpy array and matrices** - A very convinient and efective methods when we have larger number of hidden neurons or layers is by storing the data in the form of matrices and carrying out vectorized operations on them. This is many times effctive that looped operations especially when dealing with problem like building a neural network and dealing with datasets of that size.   \n",
    "- **Vectorized multiplicaion** - As mentioned above, run and training time of the progrm was drastically reduced using these opeartions. \n",
    "- **Reducing the feature size** - We tried to observe those pixels that had no information in them and try to eliminate them. But it didn't make much of a difference in our case, so we just left this option out. but reducing the input size is a good option when you have really big feature vector size because when we are acheiving better results with reducing the complexity of the data, then why not?. \n",
    "- Also, initializing the weights using smaller values proved beneficial especially for when using Batch technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2Q. MLP Architecture**<br>\n",
    "Our Multi-layered perceptron has three layers. The first one is the input layer which has 784 units, i.e., feature vectors as the input for the neural network initialization. the seond layer is hidden layer which had  500 hidden neurons in it. We were just curious as to see what happens as we consider different values for the hidden neurons. Also, viewed a few links where there were some thumb rules for considering the number of hidden neurons. The link will be in the reference section below. And the third layer that is the output layer has about 10 units in it. Because the problem we are dealing with is a 10-class classification problem. \n",
    "\n",
    "The learning rate that we considered was 0.5. \n",
    "Weight decay - We didn't consider any weight decay as we didn't find any problems in the weight update without considering it too. \n",
    "\n",
    "The upper limit on epochs we considered was 15 for sequential and 2000 for batch. For sequential, 10 epochs, 13 epochs and as well as 15 epochs showed not much difference in the results. So we didn't train the algorithm after 15 epochs.\n",
    "\n",
    "The training size was 31000 observations, the validation was 9000 observations and the test size awas 2000 observations. \n",
    "\n",
    "Randomly initialized runs - We initialized only once.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Some of the problems that we faced when building the neural network were:**<br>\n",
    "\n",
    "    -We faced overflow problem for both sequentaial and the batch technique when we trained the model using the unscaled data. This problem wsa solved for the sequential technique when we scaled the data but for the batch technique it persisted. \n",
    "    - Later on we found out that weights we considered for the batch were considerably large. Reducing the size of our initialized weights solved it. \n",
    "    - In our case, the sequential technique took longer time to train and update when compared to the batch technique but the accuracy of the batch was really far better when compared to the batch technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3Q. What's the effectiveness of the classifier.**<br> \n",
    "Our classifier after training on our training data gave 97.3% accuracy and when tested on the testing data gave 96.2% accuracy. \n",
    "\n",
    "For 10 epochs the training accuracy was 97.3% and for 15 epochs the accuracy was about 97.32%. \n",
    "Batch gave about 50% accuracy when it was tested. It did worse when compared to the sequential technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4Q. How long does it take to train the model.**  <br>\n",
    "\n",
    "Depending te size of the training data that we considred and the number of iterations we made it run, our model takes anywhere in between 1 to 1.5 hours to learn the input data. But the batch operation was a bit faster than the sequential one but the accuracy is better for the sequential technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5Q. Did you choose batch, sequential, or mini-batch training? Why?**<br>\n",
    "The methods we chose to train the data were batch and sequential. No particular reason why we chose these. We felt like these two techniques are the extremes in model training. One method considers the whole data as a batch in one iteration whereas the other takes one datapoint at a time. We also wanted to try mini-batch but couldn't because of the time constraint. So we decided mini-batch would be for futher investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Some analysis and discussion of the results obtained from the MLP**<br>\n",
    "The main goal of this project was to obtain those perfect weights that will can be used to predict our digits accurately. So, we randomly initialized our weights with smaller values, somewhere in between 0 and 1. \n",
    "As we keep iterating the network, we forward propagate the network using these weights and the input values to caluate the output value of the neural network. But as those values are not the optimal ones, we backpropagate it by calculating the errors for the total network, the output units and the hidden units. And by using these values as reference we keep updating the weights. All this is done several times in a loop till we get obtain those near perfect weights that can be used to predict the labels on our test dataset.  And then we calculate the errors and the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c06cd893755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/achyutganti/Downloads/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"error.PNG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = \"/home/achyutganti/Downloads/\"\n",
    "Image(filename = PATH + \"error.PNG\", width=4000, height=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refernce Links\n",
    "\n",
    " - https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
